# 这是用来记录项目的文档

- 现有的内容

    1. 采用对抗训练的视觉特征生成网络已经有了，但训练方式对不上
    2. 数据读取的代码已有，但还没有理解顺
    3. 有提取好的视觉特征数据

- 总体计划
    1. 确定整个模型分为几个部分来实现，以及各部分之间的关系
    2. 各部分网络的细节和具体结构，涉及到的问题
    3. 确定训练方式，各个部分的网络分别采用什么策略进行训练，分步骤说明
    4. 损失衡量
    5. 数据的来源和形式
    6. 随时记录想起的问题

## WEEK8

### Week8 Day3 2-27

整个模型分为三部分：视觉特征提取模块、基于语义特征的视觉特征生成模块、视觉特征反响生成语义特征模块。真的需要自己实现的只有视觉特征生成网络和语义特征生成网络

- 视觉特征生成网络包括：

- 语义特征生成网络包括：

### Week8 Day4 2-28

- 基于语义特征的视觉生成网络：（唯一与fxGAN不同的地方是训练方式，需要根据训练策略改变网络的架构）
    1. 噪声(正太分布、维度与fxGAN相同)+属性(与选取的视觉特征对应)作为网络的输入
    2. 网络结构采用同fxGAN相同的网络结构

### Week8 Day5 3-1

方案变化：**根据讨论，先进行简单的网络添加看效果。**

- 添加视觉特征生成回语义特征的网络
    1. 网络结构采用和对抗网路的Generator相同的结构
    2. 损失采用原始属性和生成属性之间的cos距离，将这一项添加到原来的总损失项上
    3. 训练方式采用跟随生成对抗网络的方式：在取每一个batch训练完后，直接用训练得到的Generator来生成视觉特征，然后用这些视觉特征经过R网络得到属性向量

- 直接跑实验得出一个对比效果

代码中的一个问题是：Discriminator的输入数据包括视觉特征和标签，而*标签采用的是属性向量*

### Week8 Day6 3-2

和老师交流内容：

1. 首先记录原先模型的效果
2. 加入视觉特征到属性特征的生成网络，跑出结果
3. 将判别网络的标签修改为普通的0、1作为标签
4. 将判别网络的标签修改为类别标签
5. 对上面两种标签种都进行反向生成
6. 对比有无属性生成网络的结果，对比三种不同判别器标签的结果

接下来这周先实现第4点，真的弄清晰整个代码

### Week8 Day7 3-3

原始的fxGAN的结果没跑出来，猜测问题出在最后的生成视觉特征然后进行分类的部分。

## WEEK9

### Week9 Day1 3-4

没时间弄

### Week9 Day2 3-5

查看了代码，但是由于没有空闲服务器资源，没有找到原因
查找到用生成的视觉特征训练分类器哪里，接下来是看val()函数是不是有问题

### Week9 Day3 3-6

再次通读整个网络，但没有服务器资源，没能找到问题在哪里

### Week9 Day4 3-7

智障一般的低级错误，在跑代码前**环境**都没有注意就开始跑

### Week9 Day5 3-8

加入简单的R网络，成功跑出了同参考论文水平的结果，相比自己用不加R网络的原始代码跑的结果好一些？

现有的想法：

1. 首先要确认加入属性生成网络后，确实能提升性能。
2. 更改网络结构，对比实验结果：
    - R网络结构更改的实验结果对比
3. 原始模型中分类网络是否存在对模型的影响
4. 生成对抗部分里判别器的标签更改，以及选择不同标签的原因

接下来一周时间的工作：

1. 先针对现有的模型进行准确率的对比
    - 将原始代码调试，跑出论文中陈述的结果
    - 跑加入R网络的模型的结果，并与不加R网络的结果对比，确认循环生成这条路可以走
2. 更改R网络的结构
3. 去掉分类网络保留R网络，将实验结果与原始网络对比
4. 更改判别器的标签

### Week9 Day6 3-9

## Week10

### Week10 Day1 3-11

跑完原始结果和加入R网络后的结果

### Week10 Day2 3-12

配置git仓库，将项目进行管理，以便在之后对比不同版本的网络结构

梳理下接下来几天的工作：（针对简单的R网络的实验）

R网络的作用是：基本的想法是希望，训练的生成器可以生成视觉特征了，通过将生成的视觉特征再生成回语义特征（属性）来实现类似于clsgan中的cls的作用，以强化G生成的特征确实生成了与原始属性相关的内容。

1. R网络的具体结构（用什么类型的网络/借鉴，参数的设置依据）
2. R网络如何训练（在代码那个地方进行训练，训练的数据哪里来，是否进行预训练，用什么损失，损失具体加到哪里，训练完怎么用来生成）

### Week10 Day3 3-13

R网络存在的目的是实现从生成的视觉特征再生成回属性的过程，最终是为G网络具有更好的生成能力服务的，所以R网络的更改应该以提升G网络为方向。

### Week10 Day4 3-14

### Week10 Friday 3-15

组会：

1. R网络预训练
2. 去掉分类的网络可以尝试，但目测效果不好
3. 将用于R网络输入的视觉特征经过一个判别，好的视觉特征才哪来去生成

接下来一周的工作：

1. 给R网络损失加上一个超参数实验，在CUB数据集上跑
2. 修改R网络的隐藏层数目和各隐藏层的维度看结果
3. 先进行R网络预训练看效果（预训练用数据，在整体网络之前进行预训练，修改损失函数以及损失函数加的位置）
4. 实现对R网络生成的视觉特征的筛选使用

### Week10 Saturday 3-16

给R网络的损失加上一个惩罚项，用CUB数据集来对比惩罚项大小对结果的影响

### Week10 Saturday 3-17

在cycle_weight设置为0.1时，在CUB上的最高准确率提升了，值得尝试

首先要搞清楚各个损失的含义

## Week11

### Week11 3-18 Monday

### Week11 3-19 Tuesday

修改R的网络结构：（在CUB上进行实验）

1. 单隐藏层的节点数改小，4096->2048
2. 使用双隐藏层

### Week11 3-20 Wednesday

继续针对R网络的结构进行实验，并对比结果：

1. 确定网络层数
2. 确定网络节点数

现有的四个数据集的属性个数是不一致的，应该针对各个数据集采用不同的网络结构

TODO：

1. 还是没有搞懂网络的损失
2. 需要弄明白各项损失的变化情况

### Week11 3-21 Thursday

DONE：

1. 因为使用cosine距离来算距离是，errR直接飙到0.99，所以改为2范数来算生成的属性向量的距离
2. 对CUB数据集采用2048-1024-512-312的网络结构

### Week11 3-22 Friday

组会：

1. 对R网络做预训练，再看是不是有提升
2. 给R网络添加dropout，修改网络训练用的batch
3. 对生成的att直接进行分类操作，构成一项损失
4. 将errR加到D网络损失上来训练

接下来一周的工作：

1. 精读WGAN的论文，理解WGAN的解决的问题，以及WGAN的训练过程
2. 分别对四个数据集，修改R网络结构（是否层数多会好，还是说没有影响），进行可视化对比
3. 再单独对四个数据集加上R损失的超参数，看结果
4. 对R网络进行预训练

PLAN:

- CUB数据集：加R网络2048-1024-512-312结果，加超参数结果，加dropout结果，去网络层数结果
- 了解下pytorch的可视化，把上述结果可视化
- 如果上述修改有提升就针对提高的点继续进行细致实验
- 在跑实验的时候看WGAN的博客

DONE:

- nothing，一直在学习如何使用visdom进行结果可视化

### Week11 3-23 Saturday

DAY PLAN:

- 解决使用visdom进行结果可视化（先看tensorboard如何进行本地查看，类比使用visdom）

DONE:

- 服务器上docker权力限制，不能直接进行可视化
- 能够手动处理数据进行可视化

### Week11 3-24 Sunday

DAY PLAN:

- CUB数据集：加R网络2048-1024-512-312结果，加超参数结果，加dropout结果，去网络层数结果
- 在跑实验的同时理解WGAN

## Week12

### Day1 3-25

### Day2 3-26

DAY PLAN:

- 首先是对CUB数据集实验dropout加在R网络上
- 然后把剩下的三个数据集修改R网络结构的结果跑出来
- 还有时间的话，对各个数据集的实验结果对比一下，看看有没有提升

DAY DONE:

nothing

### Day3 3-27

DAY DONE:

- CUB上跑加入dropout的R网络
- 对FLO数据集跑同样的实验

### Day4 3-28

DAY PLAN:

不能就真的想着撞大运，单纯的想试出好结果

DAY PLAN:

- 加R网络和不加R网络对比
- R单层网络和双层网络的对比
- R网络加dropout对比

### Day5 3-29

组会：

- 使用归一化的cos距离
- 在gan的网络上采用dropout，确认是在R上加dropout提升了结果
- 对R网络生成的att进行分类

接下来一周的工作：

- 对每个数据集，确定下来网络结构
- 使用cos距离和验证dropout
- 对R网络进行预训练
- 直接对生成的att进行分类，看生成的att的质量，以及提高它是否对最终分类结果有用

DAY PLAN:

- 修改R网络结构，跑flo数据集和sun数据集
- 使用归一化的cos距离

DAY DONE:

- 修改R网络，跑了flo数据集和sun数据集

### Day6 3-30

DAY PLAN:

- 测试归一化的cos距离
- 实验不同的dropout值
- 确定不同数据集对应的R网络结构，准备对生成的属性进行分类操作
- 跑实验的同时尝试自动化换参数过程

DAY DONE:

- 没有空余的GPU，没跑

### Day7 3-31

plan:

- 先使用归一化的cosine损失跑cub数据集
- 确定不同数据集所对应的最优R网络层数

done:

- 在使用cosineEmbeddingLoss时出现了torch.eq输入数据类型不匹配的问题，暂时解决不了
- 暂时确定各个数据集对应的R网络结构
  - CUB 2048-1024-512-312
  - FLO 2048-4096-1024
  - SUN 2048-1024-512-312-102
  - AWA 2048-1024-512-312-156-85

## Week14

### Day1 4-1

一天的课，晚上做算法题，背单词

### Day2 4-2

plan:

- 对SUN数据集采用4层的R网络进行实验对比，确认FLO数据集的R网络结构

done:

- 确定各数据集对应的R网络结构

### Day3 4-3

plan:

- 实验dropout值（0.5，0.3）以及对dropout效果验证（直接使用原代码已有的网络）

done:

- dropout值爲0.2時依舊取得最優的實驗結果

### Day4 4-4

组会：

- 进行R网络的预训练
- 跑gzsl的结果
- 采用不同的分类器（传统的zsl方法，用svm进行最后的分类）
- 采用生成的att作为分类基础来进行类别识别

接下来一周的工作：

1. 对R网络进行预训练后，进行实验结果
2. 跑gzsl的结果

### Day5 4-5

放假休息

### Day6 4-6

R网络的预训练:

- R网络输入GAN生成的视觉向量，将网络的结果和生成视觉向量时作为条件的真实属性向量之间的差距作为损失，以期望能通过生成的视觉向量来很好的重构回属性特征
- 预训练数据：视觉特征和属性都采用当前batch的数据（通过真实的视觉特征和对应的属性，模拟如果生成的视觉特征足够好，那么就应该通过视觉特征到属性的生成，生成出好的属性特征）
- 在哪里进行预训练：在D网络之前

### Day7 4-7

## Week15

### Day1 4-8

换一种方式实现：直接对每种要跑的参数写出shell文件，然后在另一个python文件里循环跑完对应的文件夹的文件

### Day2 4-9

plan:

把昨天写的自动化代码跑通

### Day3 4-10

实现r网络预训练

### Day4 4-11

plan：

完全整理好自动跑代码和r预训练过程

- r预训练过程中损失函数的问题
- 自动跑代码需要的shell文件
- 跑起来看结果是否符合预期

done:

一个都没解决

### Day5 4-12

### Day6 4-13

组会：

- 搞定R网络的预训练，如果损失降不下去就用降不下去的
- 跑出来预训练后的zsl和gzsl的结果，还有没有预训练的gzsl的结果
- 准备对生成的att进行分类

done:

- 用没有降损失的R预训练结果来跑，没改进

### Day7 4-14

plan:

- 找R网络预训练损失降不下去的原因

## 16

### Day4 4-18

组会：

- 预训练没有提升，不做预训练
- 仔细整理好到目前为止做的工作
- 实现对生成的att进行分类
- 看learning to compare论文，将原来网络的cls部分换掉

### Day5 4-19

plan:

- 整理之前的工作：梳理一遍代码，再跑一遍没有经过预训练的实验
- 思考对att分类需要那些工作，开始写代码

### Day6 4-20

plan:

- 去掉cls模块
- 考虑对那几种类型空间进行实验，以及多模态实验
- 基于老师提出的网络，有什么可以提出的工作
- 下午有空的话，读下learning to compare论文

done:

工作计划变更：基于对整个网络结构的思考，加r网络的目的无非就是强制让生成的特征在语义空间相匹配，而原本的cls也是为了使生成的特征在类别空间匹配，只是大家都进行了往回生成的一个过程

那么我应该做的工作，或者说比较的点，应该是去掉所有的除wgan以外的部分，然后尝试各种单模态/多模态的反向生成匹配，然后根据实验的效果来探究创新点：

- 原本的cls网络，在类别空间进行匹配
- 采用r网络，在属性语义空间进行匹配
- 对于老师提的网络，在视觉特征空间进行匹配

关于learning to compare(RN)：

1. learning to compare可以看作是一种分类的策略，可以用于最后的结果分类
2. 按文中的解释，relation module是一种更优越的距离函数，能给出更加切合数据的损失

怎么用RN：

1. 作为距离函数：
2. 作为最终的分类网络：

### Day7 4-21

plan:

- 看能不能搭出和cuda和pytorch版本兼容的环境
- 跑不同batch大小的实验
- 整理好单纯用wgan做零样本的代码

## Week17

### Day1 4-22

一天的课，晚上安装好了对应的pytorch版本

### Day2 4-23

plan：

- 环境还存在一点问题，今天需要调整好（python3.6.8 pytorch0.3.0）
- 然后把batch代码跑完，还有单纯wgan的结果
- 准备好接下来多模态的shell文件

### Day3 4-24

plan:

- 整理不同batch的结果
- 跑完纯wgan结果
- 分别整理好属性空间对比和视觉空间对比的代码

### Day4 4-25

组会：

实验初步表明生成能力几乎来自wgan，不过加上R比加上cls的效果还好一点

接下来：

1. 仔细看下learning to compare的relation module，把它用过来
2. 查看最新的gan的研究，尝试生成能力更加好的gan架构

plan:

- 仔细理解最终测试部分的代码
- 整理出单纯wgan的代码
- 看relation module论文

### Day5 4-26

plan：

- 仔细阅读learning to compare的文章，思考如何使用到自己的模型上来
- 把batch大小范围定下来

### Day6 4-27

昨天对batch的实验结果：（wgan，epoch500，其他参数默认）

- cub最优batch：512
- flo最优batch：1024 epoch399
- sun最优batch：521 epoch150
- awa最优batch：1024 epoch210

plan:

- 根据得到的batch选择，跑rwgan对应的结果
- 看度量学习的论文

## Week 18

### Day 2 4-30

组会：

- 采用multi损失：（1）当前类别的生成属性与其他类别的生成属性比较/目的是同类数据尽量靠拢（2）当前类别的生成属性与真实属性比较/目的是尽量靠近真实数据

### Day 7 5-5

plan：

- 看论文，思考度量学习如何使用，用到生成的属性上还是生成的视觉特征上

## Week 19

### Day4 5-9

plan:

- 整理出relation module的输入输出究竟是什么，究竟能不能用到我的问题上
- 如果能，实现一个简单版本

done:

- 把需要的数据给写好，最简单的不用网络，直接相减的损失都不是那么好算

### Day5 5-10

组会：依旧是R网络的损失函数的替换

- 把metric learning先用起来
- 思考如何简单的实现同类近，异类远的思想